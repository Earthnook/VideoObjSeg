{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the performance of a trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import DataParallel\n",
    "\n",
    "from vos.models.STM import STM\n",
    "from vos.algo.stm_train import STMAlgo\n",
    "from vos.models.EMN import EMN\n",
    "from vos.algo.emn_train import EMNAlgo\n",
    "from vos.datasets.DAVIS import DAVIS_2017_TrainVal\n",
    "from vos.utils.helpers import stack_images, stack_masks\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "davisroot = \"/p300/videoObjSeg_dataset/DAVIS-2017-trainval-480p\"\n",
    "videod = DAVIS_2017_TrainVal(davisroot, mode= \"val\")\n",
    "dataloader = DataLoader(videod)\n",
    "print(len(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DataParallel(STM())\n",
    "algo = STMAlgo(eval_step_kwargs= dict(Mem_every= 5),)\n",
    "\n",
    "# model = DataParallel(EMN())\n",
    "# algo = EMNAlgo(eval_step_kwargs= dict(Mem_every= 5),)\n",
    "\n",
    "model.cuda()\n",
    "algo.initialize(model)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start evaluating and calculating the mean loss and performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test():\n",
    "    loss, IoU, contour_acc = 0.,0.,0.\n",
    "    IoU_each_frame, contour_acc_frame = [], []\n",
    "    for i, data in enumerate(dataloader):\n",
    "        eval_info, extra_info = algo.eval(i, data)\n",
    "        loss += eval_info.loss\n",
    "        IoU += eval_info.IoU\n",
    "        contour_acc += eval_info.contour_acc\n",
    "        IoU_each_frame.append(extra_info[\"IoU_each_frame\"])\n",
    "        contour_acc_frame.append(extra_info[\"contour_acc_frame\"])\n",
    "        print(f\"Evaluating at itereation: {i+1},\", end= \" \")\n",
    "        print(\"IoU_this: {:.4f},\".format(eval_info.IoU), end= \" \")\n",
    "        print(\"IoU: {:.4f},\".format(IoU/(i+1)), end= \" \")\n",
    "        print(\"IoU_frame_ave: {:.4f},\".format(np.nanmean(np.hstack(IoU_each_frame))), end= \" \")\n",
    "        print(\"ContourAcc_this: {:.4f},\".format(eval_info.contour_acc), end= \" \")\n",
    "        print(\"ContourAcc: {:.4f},\".format(contour_acc / (i+1)), end= \" \")\n",
    "        print(\"ContourAcc_frame_ave: {:.4f},\".format(np.nanmean(np.hstack(contour_acc_frame))), end= \" \")\n",
    "        print(\"n_frames: {},\".format(extra_info[\"IoU_each_frame\"].shape[0]), end= \"\\r\")\n",
    "    return IoU / (i+1), \\\n",
    "        np.nanmean(np.hstack(IoU_each_frame)), \\\n",
    "        contour_acc / (i+1), \\\n",
    "        np.nanmean(np.hstack(contour_acc_frame))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IoU, f_IoU, acc, f_acc = 0,0,0,0\n",
    "itr_is = []\n",
    "IoUs, f_IoUs, accs, f_accs = [], [], [], []\n",
    "\n",
    "threshold, acc_threshold = 0.69, 0.74\n",
    "# threshold, acc_threshold = 0.57, 0.62\n",
    "# threshold, acc_threshold = 0.38, 0.47\n",
    "while (IoU < threshold and f_IoU < threshold) or (acc < acc_threshold and f_acc < acc_threshold):\n",
    "    torch.cuda.empty_cache()\n",
    "    try:\n",
    "        state_dict = torch.load(os.path.join(statedict_root, \"params.pkl\"))\n",
    "#         state_dict = torch.load(\"/root/VideoObjSeg/data/weightfiles/STM_5ImgData_fulltrain_71.1-74.2_DAVIS2017val.pkl\")\n",
    "#         state_dict = torch.load(\"/root/VideoObjSeg/data/weightfiles/EMN_5ImgData_pretrain_71.11-41.20_DAVIS2017val.pkl\")\n",
    "    except:\n",
    "        print(\"Error, continuing\", end= \"\\r\")\n",
    "        continue\n",
    "    print(\"Train iteration: \", state_dict['itr_i'])\n",
    "    model_state_dict = state_dict['model_state_dict']\n",
    "    model.load_state_dict(model_state_dict)\n",
    "    model.eval()\n",
    "    IoU, f_IoU, acc, f_acc = run_test()\n",
    "    \n",
    "#     itr_is.append(state_dict['itr_i'])\n",
    "#     IoUs.append(IoU); f_IoUs.append(f_IoU); accs.append(acc); f_accs.append(f_acc)\n",
    "#     plt.plot(itr_is, IoUs, \"b+\")\n",
    "#     plt.plot(itr_is, f_IoUs, \"bo\")\n",
    "#     plt.plot(itr_is, accs, \"r+\")\n",
    "#     plt.plot(itr_is, f_accs, \"ro\")\n",
    "    \n",
    "    print(\"Testing result IoU: {:.5f}\".format(IoU), \n",
    "          \"frames_IoU: {:.5f}\".format(f_IoU), \n",
    "          \"ContourAcc: {:.5f}\".format(acc), \n",
    "          \"frames_ContourAcc: {:.5f}\".format(f_acc), \n",
    "          \"######################################################################################################\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst = os.path.join(statedict_root, \"params-{:.2f}-{:.2f}.pkl\".format(f_IoU*100, f_acc*100))\n",
    "print(\"Save to: \", dst)\n",
    "torch.save(state_dict, dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use evaluating benchmark to evaluate model\n",
    "NOTE: you can run the following block without any of previous blocks\n",
    "\n",
    "please install https://github.com/suhwan-cho/davis-evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Space-time Memory Networks: initialized.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "from davis2017.evaluation import DAVISEvaluation\n",
    "\n",
    "############### all eval_DAVIS imports\n",
    "import torch\n",
    "from torch.utils import data\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# general libs\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import math\n",
    "import tqdm\n",
    "import threading\n",
    "\n",
    "### My libs\n",
    "from vos.datasets.DAVIS import DAVIS_MO_Test\n",
    "from vos.models.STM import STM\n",
    "from vos.algo.stm_train import STMAlgo\n",
    "from vos.models.EMN import EMN\n",
    "from vos.algo.emn_train import EMNAlgo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU = \"2\"\n",
    "YEAR = \"17\"\n",
    "SET = \"val\"\n",
    "\n",
    "davisroot = \"/p300/videoObjSeg_dataset/DAVIS-2017-trainval-480p\"\n",
    "outputroot = \"/p300/VideoObjSeg_data/STM_test/continue_test/\"\n",
    "dataset_eval = DAVISEvaluation(davis_root=davisroot, task=\"semi-supervised\", gt_set=\"val\", type=\"2017\")\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = GPU\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "palette = Image.open(davisroot + '/Annotations/480p/blackswan/00000.png').getpalette()\n",
    "\n",
    "Testset = DAVIS_MO_Test(davisroot, resolution='480p', imset='20{}/{}.txt'.format(YEAR,SET), single_object=(YEAR==16))\n",
    "Testloader = data.DataLoader(Testset, batch_size=1, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "model = nn.DataParallel(STM())\n",
    "algo = STMAlgo() # only use its step() method, so no need for any hyper-parameters\n",
    "\n",
    "# model = nn.DataParallel(EMN())\n",
    "# algo = EMNAlgo() # only use its step() method, so no need for any hyper-parameters\n",
    "\n",
    "model.cuda()\n",
    "model.eval() # turn-off BN\n",
    "algo.initialize(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "statedict_root = None\n",
    "################################# 3 datasets stm pretrain\n",
    "# statedict_root = \"/p300/VideoObjSeg_data/local/video_segmentation/20200319/synth0.0-10.0-0.05-0.0-0.1/img_res-384,384/NNSTM/big_objects-True1/b_size-4/run_0\"\n",
    "## stm fulltrain\n",
    "# statedict_root = \"/p300/VideoObjSeg_data/local/video_segmentation/20200326/img_res-384,384/NNSTM/b_size-4/pretrainTrue/run_0\"\n",
    "# statedict_root = \"/p300/VideoObjSeg_data/local/video_segmentation/20200331/img_res-384,384/NNSTM/b_size-4/pretrainTrue/run_0\"\n",
    "# statedict_root = \"/p300/VideoObjSeg_data/local/video_segmentation/20200402/img_res-384,384/NNSTM/b_size-4/pretrainTrue/run_0\"\n",
    "################################## stm main only\n",
    "# statedict_root = \"/p300/VideoObjSeg_data/local/video_segmentation/20200326/img_res-384,384/NNSTM/b_size-4/pretrainFalse/run_0\"\n",
    "# statedict_root = \"/p300/VideoObjSeg_data/local/video_segmentation/20200403/img_res-384,384/NNSTM/b_size-4/pretrainFalse/run_0\"\n",
    "################################## 5 datasets stm pretrain\n",
    "# statedict_root = \"/p300/VideoObjSeg_data/local/video_segmentation/20200327/synth0.0-10.0-0.05-0.0-0.1/img_res-384,384/NNSTM/big_objects-True1/b_size-4/run_0\"\n",
    "# statedict_root = \"/p300/VideoObjSeg_data/local/video_segmentation/20200327/synth0.0-10.0-0.05-0.0-0.1/img_res-384,384/NNSTM/big_objects-True1/b_size-4/w_decay-0.0005/run_0\"\n",
    "# statedict_root = \"/p300/VideoObjSeg_data/local/video_segmentation/20200412/synth0.0-10.0-0.05-0.0-0.1/NNSTM/trainParam-24-1e-05-100000000000000000000-0.9/pixel_dilate-1/run_0\"\n",
    "# statedict_root = \"/p300/VideoObjSeg_data/local/video_segmentation/20200413/synth0.0-10.0-0.05-0.0-0.1/NNSTM/trainParam-4-1e-05-10000000000-0.9/pixel_dilate-1/run_0\"\n",
    "# statedict_root = \"/p300/VideoObjSeg_data/local/video_segmentation/20200413/synth0.0-10.0-0.05-0.0-0.1/NNSTM/trainParam-4-5e-05-10000000000-0.9/pixel_dilate-1/run_0\"\n",
    "################################## 5 datasets emn pretrain\n",
    "# statedict_root = \"/p300/VideoObjSeg_data/local/video_segmentation/20200402/synth0.0-10.0-0.05-0.0-0.1/img_res-384,384/NNEMN/big_objects-True1/b_size-4/w_decay-0.0/run_0\"\n",
    "# statedict_root = \"/p300/VideoObjSeg_data/local/video_segmentation/20200407/synth0.0-10.0-0.05-0.0-0.1/img_res-384,384/NNEMN/big_objects-True1/b_size-4/w_decay-0.0/run_0\"\n",
    "# statedict_root = \"/root/VideoObjSeg/data/local/video_segmentation/20200409/synth0.0-10.0-0.05-0.0-0.1/NNEMN/big_objects-True1/pixel_dilate-1/run_0\"\n",
    "# statedict_root = \"/root/VideoObjSeg/data/local/video_segmentation/20200411/synth0.0-10.0-0.05-0.0-0.1/NNEMN/trainParam-24-5e-05-10000000000-0.9/pixel_dilate-1/run_0\"\n",
    "################################## emn fulltrain\n",
    "# statedict_root = \"/root/VideoObjSeg/data/local/video_segmentation/20200406/img_res-384,384/NNEMN/b_size-4/pretrainTrue/run_0\"\n",
    "# statedict_root = \"/root/VideoObjSeg/data/local/video_segmentation/20200409/NNEMN/b_size-4/pretrainTrue/run_0\"\n",
    "# statedict_root = \"/root/VideoObjSeg/data/local/video_segmentation/20200412/NNEMN/train_spec-20-5e-05/pretrainTrue/run_0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_save = [None, None, None] # a global object that enables multi-threading saving files\n",
    "def save_result_to_output():\n",
    "    seq_name, num_frames, pred = to_save[:]\n",
    "    # save elements into outputroot sub-directories\n",
    "    test_path = os.path.join(outputroot, seq_name)\n",
    "    if not os.path.exists(test_path):\n",
    "        os.makedirs(test_path)\n",
    "    for f in range(num_frames):\n",
    "        img_E = Image.fromarray(pred[f])\n",
    "        img_E.putpalette(palette)\n",
    "        img_E.save(os.path.join(test_path, '{:05d}.png'.format(f)))\n",
    "    \n",
    "def eval_davis(model_state_dict):\n",
    "    torch.cuda.empty_cache()\n",
    "    model.load_state_dict(model_state_dict)\n",
    "\n",
    "    code_name = 'DAVIS_{}{}'.format(YEAR,SET)\n",
    "    print('Start Testing: {}, output to: {}'.format(code_name, outputroot))\n",
    "\n",
    "    # construct fist thread, just for code consistency\n",
    "    global to_save\n",
    "    saving_thread = threading.Thread()\n",
    "    saving_thread.start()\n",
    "    \n",
    "    # mantain the compute and saving schema\n",
    "    for seq, V in enumerate(Testloader):\n",
    "        torch.cuda.empty_cache()\n",
    "        Fs, Ms, num_objects, info = V\n",
    "        seq_name = info['name'][0]\n",
    "        num_frames = info['num_frames'][0].item()\n",
    "        print('video: {:2d} [{:15s}]: num_frames: {:3d}, num_objects: {:2d}'.format(seq, seq_name, num_frames, num_objects[0][0]),end= \"\\r\")\n",
    "\n",
    "        # compute\n",
    "        with torch.no_grad():\n",
    "            pred, _ = algo.step(\n",
    "                frames= Fs,\n",
    "                masks= Ms,\n",
    "                n_objects= num_objects,\n",
    "                Mem_every=5, Mem_number=None\n",
    "            )\n",
    "        pred = np.argmax(pred[0].detach().cpu().numpy(), axis= 1).astype(np.uint8)\n",
    "        \n",
    "        # save\n",
    "        saving_thread.join()\n",
    "        del saving_thread\n",
    "        to_save.pop(); to_save.pop(); to_save.pop()\n",
    "        to_save.extend([seq_name, num_frames, pred])\n",
    "        saving_thread = threading.Thread(target= save_result_to_output)\n",
    "        saving_thread.start()\n",
    "        \n",
    "    saving_thread.join()\n",
    "    del saving_thread\n",
    "    print(\"Output to {} done\".format(outputroot))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "J_thresh, F_thresh = 0.57, 0.62\n",
    "J_mean, F_mean = 0, 0\n",
    "while J_mean < J_thresh and F_mean < F_thresh and False:\n",
    "    # run to generate val output\n",
    "    try:\n",
    "        print(\"Loading weights\", end= \"\\r\")\n",
    "#         state_dict = torch.load(os.path.join(statedict_root, \"params.pkl\"))\n",
    "#         state_dict = torch.load(\"/root/VideoObjSeg/data/weightfiles/EMN_5ImgData_pretrain_71.11-41.20_DAVIS2017val.pkl\")\n",
    "#         state_dict = torch.load(\"/root/VideoObjSeg/data/weightfiles/STM_5ImgData_fulltrain_71.1-74.2_DAVIS2017val.pkl\") # \n",
    "#         state_dict = torch.load(\"/root/VideoObjSeg/data/weightfiles/STM_5ImgData_pretrain_62.1-65.6_DAVIS2017val.pkl\")\n",
    "#         state_dict = torch.load(\"/root/VideoObjSeg/data/weightfiles/STM_params_56.38DAVIS2017val.pkl\")\n",
    "        state_dict = torch.load(\"/root/VideoObjSeg/data/weightfiles/STM_pretrain_62.29-65.62_DAVIS2017val.pkl\")\n",
    "        print(\"Weight loaded at itr: {}\".format(state_dict[\"itr_i\"]))\n",
    "        eval_davis(state_dict[\"model_state_dict\"])\n",
    "    except Exception as e:\n",
    "        print(e, end= \"\\r\")\n",
    "        continue\n",
    "    # run to evaluate the output\n",
    "    print(\"Evaluating using evaluating server\")\n",
    "    metric_res = dataset_eval.evaluate(outputroot)\n",
    "    J, F = metric_res['J'], metric_res['F']\n",
    "    J_mean, F_mean = np.mean(J['M']), np.mean(F['M'])\n",
    "    print(\"J_mean: {:.4f}, F_mean: {:.4f}\".format(J_mean, F_mean))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst = os.path.join(statedict_root, \"params-{:.2f}-{:.2f}.pkl\".format(J_mean*100, F_mean*100))\n",
    "print(\"Save to: \", dst)\n",
    "torch.save(state_dict, dst)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vos",
   "language": "python",
   "name": "vos"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
